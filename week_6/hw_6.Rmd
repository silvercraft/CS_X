---
title: "Text Mining:Hitler's Mein Kampf(first half)"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load the libraries.

```{r library}
library(rvest)
library(tm)
library(SnowballC)
library(RColorBrewer)
library(dplyr)
library(NLP)
library(ggplot2)
library(Matrix)


```

Crawling text from the website and deleting unneccesary words.

```{r text}

prefix = "https://www.theguardian.com/world/north-korea?page="

f=""
for(i in 1:5)
url  <- paste0( prefix, as.character(i), ".html" )
  page.source2 <- read_html(url)
  kw2 = html_nodes(page.source2, ".fc-item--type-guardianview .js-headline-text , .fc-item--type-article .js-headline-text")
  f=paste(c(f,html_text(kw2)))
 
docs <- Corpus(VectorSource(f))
toSpace <- content_transformer(function(x, pattern) {
  return (gsub(pattern, " ", x))}
)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removeWords, "\n")
docs <- tm_map(docs, removeWords, "</i>")
docs <- tm_map(docs, removeWords, "<br>")
docs <- tm_map(docs, removeWords, "\r")


```
Words frequencies for the entire text.
```{r freq}
tdm <- TermDocumentMatrix(docs)
freq=rowSums(as.matrix(tdm))
tail(sort(freq),n=10)
high.freq=tail(sort(freq),n=10)
hfp.df=as.data.frame(sort(high.freq))
hfp.df$names <- rownames(hfp.df) 


```
Now,analyse with tf-idf algorithm.
```{r tfidf}
# tf-idf computation
tf <- apply(tdm, 2, sum) # term frequency
idf <- function(word_doc){ log2( (length(word_doc)+1) / nnzero(word_doc) ) }
idf <- apply(tdm, 1, idf)
doc.tfidf <- as.matrix(tdm)
for(i in 1:nrow(tdm)){
    for(j in 1:ncol(tdm)){
        doc.tfidf[i,j] <- (doc.tfidf[i,j] / tf[j]) * idf[i]
    }
}

```

```{r kable}
n=38
tempY = matrix(rep(c(as.matrix(tf)), each = length(idf)), nrow = length(idf))
tempX = matrix(rep(c(as.matrix(idf)), each = length(tf)), ncol = length(tf), byrow = TRUE)
doc.tfidf[,2:(n+1)] <- (doc.tfidf[,2:(n+1)] / tempY) * tempX
stopLine = rowSums(doc.tfidf[,2:(n+1)])
delID = which(stopLine == 0)
kable(head(doc.tfidf[delID,1]))
kable(tail(doc.tfidf[delID,1]))
```

```{r}
TopWords = data.frame()
for( id in c(1:n) )
{
  dayMax = order(doc.tfidf[,id+1], decreasing = TRUE)
  showResult = t(as.data.frame(doc.tfidf[dayMax[1:20],1]))
  TopWords = rbind(TopWords, showResult)
}
rownames(TopWords) = colnames(doc.tfidf)[2:(n+1)]
TopWords = droplevels(TopWords)
kable(TopWords)
```


```{r}
TDM$d = as.character(TDM$d)
AllTop = as.data.frame( table(as.matrix(TopWords)) )
AllTop = AllTop[order(AllTop$Freq, decreasing = TRUE),]
kable(head(AllTop))
```



```{r}
filenames = as.array(paste0("./DATA/",colnames(doc.tfidf)[2:(n+1)],".txt"))
sizeResult = apply(filenames, 1, file.size) / 1024
showSize = data.frame(colnames(doc.tfidf)[2:(n+1)], sizeResult)
names(showSize) = c("month", "size_KB")
showSize$month <- fct_inorder(showSize$month)
ggplot(showSize, aes(x = month, y = size_KB)) + geom_bar(stat="identity")
```

