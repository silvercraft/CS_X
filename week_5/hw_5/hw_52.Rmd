---
title: "Text Mining:Hitler's Mein Kampf(first half)"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load the libraries.

```{r library}
library(rvest)
library(tm)
library(SnowballC)
library(RColorBrewer)
library(dplyr)
library(NLP)
library(ggplot2)
library(Matrix)
library(wordcloud)


```

Crawling text from the website and deleting unneccesary words.

```{r text}

prefix = "http://www.hitler.org/writings/Mein_Kampf/mkv1ch0"
prefix2 = "http://www.hitler.org/writings/Mein_Kampf/mkv1ch"
prefix3 = "http://www.hitler.org/writings/Mein_Kampf/mkv2ch0"
prefix4 = "http://www.hitler.org/writings/Mein_Kampf/mkv2ch"
f=""
data <- list()
for( id in 1:9 )
{
  url  <- paste0( prefix, as.character(id), ".html" )
  page.source2 <- read_html(url)
  kw2 = html_nodes(page.source2, "blockquote")
  f=paste(c(f,html_text(kw2)))
  
  
}
for( id in 10:12 )
{
  url  <- paste0( prefix2, as.character(id), ".html" )
  page.source2 <- read_html(url)
  kw2 = html_nodes(page.source2, "blockquote")
  f=paste(c(f,html_text(kw2)))
  
}
for( id in 1:9 )
{
  url  <- paste0( prefix3, as.character(id), ".html" )
  page.source2 <- read_html(url)
  kw2 = html_nodes(page.source2, "blockquote")
  f=paste(c(f,html_text(kw2)))
  
}
for( id in 10:12 )
{
  url  <- paste0( prefix4, as.character(id), ".html" )
  page.source2 <- read_html(url)
  kw2 = html_nodes(page.source2, "blockquote")
  f=paste(c(f,html_text(kw2)))
  
}

docs <- Corpus(VectorSource(f))
toSpace <- content_transformer(function(x, pattern) {
  return (gsub(pattern, " ", x))}
)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removeWords, "even")
docs <- tm_map(docs, removeWords, "also")
docs <- tm_map(docs, removeWords, "will")
docs <- tm_map(docs, removeWords, "can")
docs <- tm_map(docs, removeWords, "must")
docs <- tm_map(docs, removeWords, "\n")
docs <- tm_map(docs, removeWords, "</i>")
docs <- tm_map(docs, removeWords, "<br>")
docs <- tm_map(docs, removeWords, "\r")


```
Words frequencies for the entire text.
```{r freq}
tdm <- TermDocumentMatrix(docs)
freq=rowSums(as.matrix(tdm))
plot(sort(freq, decreasing = T),col="blue",main="Word frequencies", xlab="TF-IDF-based rank", ylab = "TF-IDF")
tail(sort(freq),n=10)
high.freq=tail(sort(freq),n=10)
hfp.df=as.data.frame(sort(high.freq))
hfp.df$names <- rownames(hfp.df) 


```
Ploting the words frequencies with bar chart.
```{r plot}
ggplot(hfp.df, aes(reorder(names,high.freq), high.freq)) +
  geom_bar(stat="identity") + coord_flip() + 
  xlab("Terms") + ylab("Frequency") +
  ggtitle("Term frequencies")


```
And also a wordcloud.
```{r cloud}
m1 <- as.matrix(tdm)
v <- sort(rowSums(m1), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
wordcloud(d$word, d$freq, min.freq = 10,max.words = 50, random.order = F, ordered.colors = F, 
    colors = rainbow(length(row.names(m1))))

```
Now,analyse with tf-idf algorithm.
```{r tfidf}
# tf-idf computation
tf <- apply(tdm, 2, sum) # term frequency
idf <- function(word_doc){ log2( (length(word_doc)+1) / nnzero(word_doc) ) }
idf <- apply(tdm, 1, idf)
doc.tfidf <- as.matrix(tdm)
for(i in 1:nrow(tdm)){
    for(j in 1:ncol(tdm)){
        doc.tfidf[i,j] <- (doc.tfidf[i,j] / tf[j]) * idf[i]
    }
}
```
Plot for the entire 13 chapters.We can see Hitler's change of mind over the years,which is quite interesting.
```{r tfidfcombine}
  for(i in 2:14){
  
    nn2=data.frame(tfidf=head(sort(doc.tfidf[,i],decreasing = TRUE),10))
    nn2$names<- rownames(nn2) 
    print(ggplot(nn2, aes(names,tfidf)) +
    geom_bar(stat="identity") + coord_flip() + 
    xlab("Terms") + ylab("Frequency") +
  ggtitle(paste("Chapter ",i-1)) )
  }
  
  



```

Hope you know Hitler more after seeing all these analysis (wait,what?)


